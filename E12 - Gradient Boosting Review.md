## E12 - Gradient Boosting Review
Los dos algoritmos tanto el gradient boosting classifier como el XGB classifier tienen los mismos principios iniciales de funcionamiento es decir ambos se basan en el objetivo de crear un modelo fuerte partiendo de la generación de múltiples modelos más débiles, en cada una de las iteraciones se realizan ajustes a determinada función para corregir las clasificaciones erróneas, el hecho de realizar múltiples iteraciones para corregir los errores hacen que el algoritmo base utilizado en su totalidad en el GBC llevaron a la necesidad de generar un modelo que fuera computacionalmente más optimo el cual se denomina XGB Classifier, en el se parte del mismo principio del GBC el cual genera unas predicciones más acertadas que las de los modelos débiles por si solos pero buscando solucionar los problemas computaciones que esto trae consigo, algunos de los cambios más importantes están en considerar únicamente los mejores splits de cada una de las variables para generar los árboles que se utilizan.

Adicionalmente la función de costo que utiliza el algoritmo de XGB es una simplificación obtenida a través de la expansión de Taylor de la función utilizada en GBC, en XGB también se puede tener un early stop, es decir una detención del algoritmo cuando existen variaciones no significativas en una predicción realizada sobre una base adicional de validación, lo cual evita algo el sobre ajuste. Como es perceptible la mayoría de cambios apuntan a una optimización computacional y no a variaciones de fondo en el sentido del algoritmo.

Todo esto ha derivado en una optimización computacional considerable del modelo inicial, sin perder las virtudes que tiene en cuanto al ajuste iterativo del error.

Por otro lado el algoritmo de XGB tiene una gran cantidad de hiperparámetros algo que hace que realizar ajustes sobre él sea algo complejo a comparación del GBC.
